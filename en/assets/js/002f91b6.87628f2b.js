"use strict";(self.webpackChunkathenaserving=self.webpackChunkathenaserving||[]).push([[3320],{3905:(e,n,t)=>{t.d(n,{Zo:()=>m,kt:()=>u});var r=t(7294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function p(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function s(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)t=i[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var o=r.createContext({}),l=function(e){var n=r.useContext(o),t=n;return e&&(t="function"==typeof e?e(n):p(p({},n),e)),t},m=function(e){var n=l(e.components);return r.createElement(o.Provider,{value:n},e.children)},c={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},d=r.forwardRef((function(e,n){var t=e.components,a=e.mdxType,i=e.originalType,o=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),d=l(t),u=a,g=d["".concat(o,".").concat(u)]||d[u]||c[u]||i;return t?r.createElement(g,p(p({ref:n},m),{},{components:t})):r.createElement(g,p({ref:n},m))}));function u(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var i=t.length,p=new Array(i);p[0]=d;var s={};for(var o in n)hasOwnProperty.call(n,o)&&(s[o]=n[o]);s.originalType=e,s.mdxType="string"==typeof e?e:a,p[1]=s;for(var l=2;l<i;l++)p[l]=t[l];return r.createElement.apply(null,p)}return r.createElement.apply(null,t)}d.displayName="MDXCreateElement"},5818:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>p,default:()=>c,frontMatter:()=>i,metadata:()=>s,toc:()=>l});var r=t(7462),a=(t(7294),t(3905));const i={sidebar_position:3,sidebar_label:"\u4e09\u3001\u5b9e\u73b0\u63a8\u7406\u903b\u8f91"},p="\u5b9e\u73b0\u63a8\u7406\u903b\u8f91",s={unversionedId:"\u52a0\u8f7d\u5668/\u521b\u5efawrapper/\u5b9e\u73b0\u63a8\u7406",id:"\u52a0\u8f7d\u5668/\u521b\u5efawrapper/\u5b9e\u73b0\u63a8\u7406",title:"\u5b9e\u73b0\u63a8\u7406\u903b\u8f91",description:"\u5728\u9879\u76ee\u521b\u5efa\u540e\uff0c\u7528\u6237\u53ea\u9700\u8981\u5b8c\u6210\u4e0b\u9762\u7684\u4efb\u52a1\u5373\u53ef\u5b8c\u6210\u63a8\u7406\u670d\u52a1\u7684\u5728athenaServing\u4e0a\u9002\u914d\u548c\u90e8\u7f72\u3002",source:"@site/docs/\u52a0\u8f7d\u5668/\u521b\u5efawrapper/\u5b9e\u73b0\u63a8\u7406.md",sourceDirName:"\u52a0\u8f7d\u5668/\u521b\u5efawrapper",slug:"/\u52a0\u8f7d\u5668/\u521b\u5efawrapper/\u5b9e\u73b0\u63a8\u7406",permalink:"/athena_website/en/docs/\u52a0\u8f7d\u5668/\u521b\u5efawrapper/\u5b9e\u73b0\u63a8\u7406",draft:!1,editUrl:"https://github.com/xfyun/athena_website/tree/master/docs/\u52a0\u8f7d\u5668/\u521b\u5efawrapper/\u5b9e\u73b0\u63a8\u7406.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3,sidebar_label:"\u4e09\u3001\u5b9e\u73b0\u63a8\u7406\u903b\u8f91"},sidebar:"tutorialSidebar",previous:{title:"\u4e8c\u3001\u521b\u5efa\u9879\u76ee",permalink:"/athena_website/en/docs/\u52a0\u8f7d\u5668/\u521b\u5efawrapper/\u521b\u5efa\u9879\u76ee"},next:{title:"\u56db\u3001\u4e1a\u52a1\u955c\u50cf\u6784\u5efa",permalink:"/athena_website/en/docs/\u52a0\u8f7d\u5668/\u521b\u5efawrapper/\u4e1a\u52a1\u955c\u50cf\u6784\u5efa"}},o={},l=[{value:"\u793a\u4f8b\uff1a\u901a\u8fc7Pytorch\u5b9e\u73b0MNIST\u624b\u5199\u6570\u5b57\u8bc6\u522b",id:"\u793a\u4f8b\u901a\u8fc7pytorch\u5b9e\u73b0mnist\u624b\u5199\u6570\u5b57\u8bc6\u522b",level:2},{value:"1. \u514b\u9686\u793a\u4f8b\u6a21\u578b\u670d\u52a1\u4ed3\u5e93",id:"1-\u514b\u9686\u793a\u4f8b\u6a21\u578b\u670d\u52a1\u4ed3\u5e93",level:3},{value:"2. \u67e5\u770b\u670d\u52a1\u7684\u63a8\u7406\u903b\u8f91\uff08inference.py\uff09",id:"2-\u67e5\u770b\u670d\u52a1\u7684\u63a8\u7406\u903b\u8f91inferencepy",level:3},{value:"3. \u5b89\u88c5\u670d\u52a1\u4f9d\u8d56(requirements.txt)",id:"3-\u5b89\u88c5\u670d\u52a1\u4f9d\u8d56requirementstxt",level:3},{value:"4. \u9002\u914d\u901a\u8fc7aiges\u6784\u5efa\u7684mnist\u9879\u76ee\u7ed3\u6784",id:"4-\u9002\u914d\u901a\u8fc7aiges\u6784\u5efa\u7684mnist\u9879\u76ee\u7ed3\u6784",level:3},{value:"5. \u7f16\u5199wrapper.py",id:"5-\u7f16\u5199wrapperpy",level:3},{value:"6. \u6d4b\u8bd5wrapper.py",id:"6-\u6d4b\u8bd5wrapperpy",level:3}],m={toc:l};function c(e){let{components:n,...t}=e;return(0,a.kt)("wrapper",(0,r.Z)({},m,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"\u5b9e\u73b0\u63a8\u7406\u903b\u8f91"},"\u5b9e\u73b0\u63a8\u7406\u903b\u8f91"),(0,a.kt)("p",null,"\u5728\u9879\u76ee\u521b\u5efa\u540e\uff0c\u7528\u6237\u53ea\u9700\u8981\u5b8c\u6210\u4e0b\u9762\u7684\u4efb\u52a1\u5373\u53ef\u5b8c\u6210\u63a8\u7406\u670d\u52a1\u7684\u5728athenaServing\u4e0a\u9002\u914d\u548c\u90e8\u7f72\u3002"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"\u586b\u5199\u6a21\u578b\u670d\u52a1\u7684\u8f93\u5165\u8f93\u51fa"),(0,a.kt)("li",{parentName:"ul"},"\u7f16\u5199\u6a21\u578b\u7684\u63a8\u7406\u670d\u52a1\u6d41\u7a0b\uff08\u76ee\u524d\u4ec5\u652f\u6301\u975e\u6d41\u5f0f\uff09")),(0,a.kt)("p",null,"\u6ce8\uff1a ",(0,a.kt)("inlineCode",{parentName:"p"},"\u6587\u4e2d\u9700\u8981\u4fee\u6539\u7684key\u9700\u8981\u4e0e\u521b\u5efa\u80fd\u529b\u7684key\u503c\u76f8\u540c"),"\u3002"),(0,a.kt)("h2",{id:"\u793a\u4f8b\u901a\u8fc7pytorch\u5b9e\u73b0mnist\u624b\u5199\u6570\u5b57\u8bc6\u522b"},(0,a.kt)("a",{parentName:"h2",href:"https://github.com/sea-wyq/pytorch-MNIST.git"},"\u793a\u4f8b\uff1a\u901a\u8fc7Pytorch\u5b9e\u73b0MNIST\u624b\u5199\u6570\u5b57\u8bc6\u522b")),(0,a.kt)("h3",{id:"1-\u514b\u9686\u793a\u4f8b\u6a21\u578b\u670d\u52a1\u4ed3\u5e93"},"1. \u514b\u9686\u793a\u4f8b\u6a21\u578b\u670d\u52a1\u4ed3\u5e93"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"git clone https://github.com/sea-wyq/pytorch-MNIST.git\n\nroot@34c3e20d9572:/home# tree pytorch-MNIST/\npytorch-MNIST/\n|-- 0.png                   # \u6d4b\u8bd5\u63a8\u7406\u4f7f\u7528\u7684\u624b\u5199\u6570\u5b57\u56fe\u7247\n|-- README.md \n|-- Model.py                # \u670d\u52a1\u6a21\u578b\n|-- inference.py            # \u6a21\u578b\u63a8\u7406\u903b\u8f91\n|-- requirements.txt        # \u670d\u52a1\u4f9d\u8d56\u7684\u5305\n`-- train.py                # \u6a21\u578b\u8bad\u7ec3\u903b\u8f91\n")),(0,a.kt)("p",null,"\u6267\u884c ",(0,a.kt)("inlineCode",{parentName:"p"},"python3 pytorch-MNIST/train.py"),"\u4f1a\u83b7\u5f97\u6a21\u578b\u6587\u4ef6",(0,a.kt)("inlineCode",{parentName:"p"},"mnist.pkl"),"\u548cmnist\u6570\u636e\u96c6",(0,a.kt)("inlineCode",{parentName:"p"},"data"),"\u3002 "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-shell"},"root@34c3e20d9572:/home# tree pytorch-MNIST/\npytorch-MNIST/\n|-- 0.png\n|-- Model.py\n|-- README.md\n|-- data\n|   `-- MNIST\n|       `-- raw\n|           |-- t10k-images-idx3-ubyte\n|           |-- t10k-images-idx3-ubyte.gz\n|           |-- t10k-labels-idx1-ubyte\n|           |-- t10k-labels-idx1-ubyte.gz\n|           |-- train-images-idx3-ubyte\n|           |-- train-images-idx3-ubyte.gz\n|           |-- train-labels-idx1-ubyte\n|           `-- train-labels-idx1-ubyte.gz\n|-- inference.py\n|-- mnist.pkl\n|-- requirements.txt\n`-- train.py\n")),(0,a.kt)("h3",{id:"2-\u67e5\u770b\u670d\u52a1\u7684\u63a8\u7406\u903b\u8f91inferencepy"},"2. \u67e5\u770b\u670d\u52a1\u7684\u63a8\u7406\u903b\u8f91\uff08inference.py\uff09"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'import cv2\nimport torch\nimport torchvision.transforms as transforms\n\nfrom Model import MNIST\n\n\ndef images2tensor(image):\n    img = cv2.imread(image)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    transf = transforms.ToTensor()\n    img_tensor = torch.unsqueeze(transf(img), dim=0)\n    return img_tensor\n\n\nif __name__ == "__main__":\n    device = torch.device(\'cpu\')\n    model = MNIST().to(device)\n    model.load_state_dict(torch.load(\'mnist.pkl\'))     # \u52a0\u8f7d\u670d\u52a1\u6a21\u578b\n    input_data = images2tensor("0.png")                # \u52a0\u8f7d\u6d4b\u8bd5\u6570\u636e\n    res = model(input_data)                         \n    print("\u624b\u5199\u6570\u5b57\u56fe\u7247\u68c0\u6d4b\u7684\u7ed3\u679c\u4e3a\uff1a", res.argmax())    # \u6253\u5370\u63a8\u7406\u7ed3\u679c\n')),(0,a.kt)("h3",{id:"3-\u5b89\u88c5\u670d\u52a1\u4f9d\u8d56requirementstxt"},"3. \u5b89\u88c5\u670d\u52a1\u4f9d\u8d56(requirements.txt)"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"cd pytorch-MNIST\npip3 install -r requirements.txt\n")),(0,a.kt)("h3",{id:"4-\u9002\u914d\u901a\u8fc7aiges\u6784\u5efa\u7684mnist\u9879\u76ee\u7ed3\u6784"},"4. \u9002\u914d\u901a\u8fc7aiges\u6784\u5efa\u7684mnist\u9879\u76ee\u7ed3\u6784"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"\u5c060.png\u79fb\u52a8\u5230mnist/wrapper/test_data\u76ee\u5f55\u4e0b"),(0,a.kt)("li",{parentName:"ul"},"\u5c06mnist.pkl\u548cModel.py\u6587\u4ef6\u653e\u5230mnist/wrapper\u76ee\u5f55\u4e0b"),(0,a.kt)("li",{parentName:"ul"},"\u5c06pytorch-MNIST\u76ee\u5f55\u4e0b\u7684requirements.txt\u6dfb\u52a0\u5230mnist/requirements.txt\u4e2d"),(0,a.kt)("li",{parentName:"ul"},"\u7f16\u5199Dockerfile(\u4e0b\u7ae0\u8282\u4ecb\u7ecd)"),(0,a.kt)("li",{parentName:"ul"},"\u7f16\u5199wrapper\u76ee\u5f55\u4e0b\u7684wrapper.py\u63a8\u7406\u903b\u8f91")),(0,a.kt)("p",null,"\u6700\u7ec8\u7684mnist\u76ee\u5f55\u7ed3\u6784\u4e3a\uff1a"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"root@34c3e20d9572:/home# tree mnist/\nmnist/\n|-- Dockerfile\n|-- README.md\n|-- requirements.txt\n`-- wrapper\n    |-- mnist.pkl\n    |-- Model.py\n    |-- test_data\n    |   `-- 0.png\n    `-- wrapper.py\n")),(0,a.kt)("h3",{id:"5-\u7f16\u5199wrapperpy"},"5. \u7f16\u5199wrapper.py"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},'#!/usr/bin/env python\n# coding:utf-8\n"""\n@license: Apache License2\n@file: wrapper.py\n@time: 2022-08-19 02:05:07.467170\n@project: mnist\n@project: ./\n"""\n\nimport sys\nimport hashlib\ntry:\n    from aiges_embed import ResponseData, Response, DataListNode, DataListCls  # c++\nexcept:\n    from aiges.dto import Response, ResponseData, DataListNode, DataListCls,Once, DataText\n\nfrom aiges.sdk import WrapperBase, \\\n    StringParamField, \\\n    ImageBodyField, \\\n    StringBodyField\nfrom aiges.utils.log import log\n\n\n# \u5bfc\u5165inference.py\u4e2d\u7684\u4f9d\u8d56\u5305\nimport cv2\nimport torch\nimport torchvision.transforms as transforms\nfrom Model import MNIST\nimport numpy as np\n\n\n# \u5b9a\u4e49\u6a21\u578b\u7684\u8d85\u53c2\u6570\u548c\u8f93\u5165\u53c2\u6570\nclass UserRequest(object):\n\n    input1 = ImageBodyField(key="img", path="test_data/0.png")\n\n# \u5b9a\u4e49\u6a21\u578b\u7684\u8f93\u51fa\u53c2\u6570\nclass UserResponse(object):\n    accept1 = StringBodyField(key="number")\n\n# \u5b9a\u4e49\u670d\u52a1\u63a8\u7406\u903b\u8f91\nclass Wrapper(WrapperBase):\n    serviceId = "mnist"\n    version = "v1"\n    requestCls = UserRequest()\n    responseCls = UserResponse()\n    model = None\n\n    # \u52a0\u8f7d\u6a21\u578b\u6587\u4ef6\n    def wrapperInit(cls, config: {}) -> int:\n        log.info("Initializing ...")\n        device = torch.device(\'cpu\')\n        cls.model = MNIST().to(device)\n        cls.model.load_state_dict(torch.load(\'mnist.pkl\'))\n        return 0\n\n    # \u7f16\u5199\u63a8\u7406\u903b\u8f91\n    def wrapperOnceExec(cls, params: {}, reqData: DataListCls) -> Response:\n\n        # \u8bfb\u53d6\u6d4b\u8bd5\u56fe\u7247\u5e76\u8fdb\u884c\u6a21\u578b\u63a8\u7406\n        log.info("got reqdata , %s" % reqData.list)\n        imagebytes = reqData.get("img").data    # \u8bfb\u53d6\u5230\u7684\u662f\u4e8c\u8fdb\u5236\u6570\u636e\n        image  = [cv2.imdecode(np.frombuffer(imagebytes, np.uint8), cv2.COLOR_BGR2GRAY)]\n        image_tensor = torch.unsqueeze(torch.Tensor(image), dim=0)\n        result = cls.model(image_tensor).argmax()\n        print(result)\n\n        # \u4f7f\u7528Response\u5c01\u88c5result\n        res = Response()\n        resd = ResponseData()\n        resd.key = "img"\n        resd.type = DataText\n        resd.status = Once\n        resd.data = result\n        resd.len = 1\n        res.list = [resd]\n\n    def wrapperFini(cls) -> int:\n        return 0\n\n\n    def wrapperError(cls, ret: int) -> str:\n        if ret == 100:\n            return "user error defined here"\n        return ""\n\n    \'\'\'\n        \u6b64\u51fd\u6570\u4fdd\u7559\u6d4b\u8bd5\u7528\uff0c\u4e0d\u53ef\u5220\u9664\n    \'\'\'\n\n    def wrapperTestFunc(cls, data: [], respData: []):\n        pass\n\nif __name__ == \'__main__\':\n    m = Wrapper()\n    m.run()\n')),(0,a.kt)("h3",{id:"6-\u6d4b\u8bd5wrapperpy"},"6. \u6d4b\u8bd5wrapper.py"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"root@34c3e20d9572:/home/mnist/wrapper# python3 wrapper.py \nFound proto: WrapperBase\nFound proto: Wrapper\n2022-08-19 03:32:06,707 - root - INFO - Initializing ...\n2022-08-19 03:32:06,885 - root - INFO - got reqdata , [<aiges.dto.DataListNode object at 0x7f6bfc7beda0>]\n/home/mnist/wrapper/wrapper.py:64: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:204.)\n  image_tensor = torch.unsqueeze(torch.Tensor(image), dim=0)\n[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\ntensor(0)\n['img']\n")))}c.isMDXComponent=!0}}]);